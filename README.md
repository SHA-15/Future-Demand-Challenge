# Future Demand Challenge

![image](https://github.com/SHA-15/Future-Demand-Challenge/assets/148129383/b71dd518-57db-48d5-9455-43817a0d8cb7)

The Tasks implements a Web Crawler and Scraper program in Python using the Selenium ğŸš€ Library to be able to parse our website's data into the POSTGRESQL DATABASE âºï¸. The following are the libraries utilized for the execution of the project:
1. Selenium ğŸ”©
2. Pandas âš¡
3. psycopg2 ğŸ“°
4. datetime & time â²ï¸

Reason for choosing Selenium over other frameworks:
ğŸ‘ Selenium excels in dealing with core javascript based web applications, but it's good for projects where speed isn't relevant.
ğŸ‘ Beautiful Soap provides greater access in parsing static HTML elements, but does not cater well towards JAVASCRIPT Features.
ğŸ‘ Selenium allows integration of JS triggers and shares similarities with DOM object interactivities such as clicking.

The Program Structure:

The entire program is broken down into 3 Distinct Elements:
1. Two-Part Data Access Process
2. Data Manipulation and Cleansing (Utilizing Pandas)
3. Data Load to PostgreSQL

![image](https://github.com/SHA-15/Future-Demand-Challenge/assets/148129383/bcaab539-f51c-49b6-9246-5683cf5abc6c)



